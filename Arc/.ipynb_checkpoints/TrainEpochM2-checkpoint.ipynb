{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "from qiskit import Aer, IBMQ\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.algorithms.optimizers import COBYLA, NELDER_MEAD, SLSQP, SPSA\n",
    "from qiskit.opflow import Z, I, StateFn, PauliExpectation, Gradient, NaturalGradient\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN, OpflowQNN, CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.neural_networks import EffectiveDimension\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC, NeuralNetworkClassifier\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "# Set seed for random generators\n",
    "algorithm_globals.random_seed = 42\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Components.circuits import preTrainedBlockGenerator, layerwise_training, featureMapGenerator, AnsatzGenerator\n",
    "from Components.data import breast_cancer, iris, fetch_mnist\n",
    "from Components.utils import plot_loss, score, parity, classification_callback, plot_objfn_range, result_to_objfun_dataframes, save_results\n",
    "from Components.train import create_qnn, sampling_experiment, train, train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('statevector_simulator')\n",
    "\n",
    "q_instance = QuantumInstance(\n",
    "    backend, \n",
    "    shots = 100, \n",
    "    seed_simulator = 2718, \n",
    "    seed_transpiler = 2718,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 350 samples\n",
      "Testing set: 150 samples\n",
      "Number of features: 4\n",
      "Classes:[0 1]; Encoded as: [-1  1]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = fetch_mnist(PCA_n = 4, data_size=500)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train_t = torch.from_numpy(y_train).to(torch.float32)\n",
    "X_val_t = torch.from_numpy(X_val).to(torch.float32)\n",
    "y_val_t = torch.from_numpy(y_val).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1000)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss(losses):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIM = X_train.shape[-1]\n",
    "max_qubit = FEATURE_DIM\n",
    "MAX_REPS = 9\n",
    "MIN_REPS = 1\n",
    "MAX_IDENTITIES_BLOCKS = 2  # <---- Number of identity blocks, depth values of identity blocks is close to that of normal ansatz (5 qubits)\n",
    "ENTANGLEMENT = 'linear'\n",
    "\n",
    "GLOBAL_OPERATOR = Z ^ FEATURE_DIM\n",
    "LOCAL_OPERATOR = (I ^ (FEATURE_DIM - 2)) ^ (Z^2) \n",
    "MAX_ITER = 120\n",
    "\n",
    "parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2  # optional interpret function, can't be used for EstimatorQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = featureMapGenerator(FEATURE_DIM)\n",
    "ansatz = AnsatzGenerator(FEATURE_DIM, reps=MAX_REPS, entanglement=ENTANGLEMENT)\n",
    "qc_2 = feature_map.compose(ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instance 9\n",
      "Epoch      Loss                 Train Accuracy   Test Accuracy   \n",
      "[ 0 ]      0.9398699998855591   0.58571          0.66            \n",
      "[ 10 ]     0.6159600019454956   0.88             0.88            \n",
      "[ 20 ]     0.5516600012779236   0.90571          0.91333         \n",
      "[ 30 ]     0.5357099771499634   0.91143          0.89333         \n",
      "[ 40 ]     0.5026100277900696   0.94286          0.88667         \n",
      "[ 50 ]     0.4791699945926666   0.95714          0.92            \n",
      "[ 60 ]     0.46898001432418823  0.96286          0.93333         \n",
      "[ 70 ]     0.46452999114990234  0.96571          0.92667         \n",
      "[ 80 ]     0.4623599946498871   0.96286          0.92667         \n",
      "[ 90 ]     0.45802998542785645  0.95714          0.92            \n",
      "[ 100 ]    0.4549899995326996   0.95714          0.90667         \n",
      "[ 110 ]    0.45333999395370483  0.96             0.9             \n",
      "[ 119 ]    0.45486000180244446  0.95714          0.9             \n"
     ]
    }
   ],
   "source": [
    "losses_2 = []\n",
    "accuracy_train_2 = []\n",
    "accuracy_test_2 = []\n",
    "weights_2 = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Training instance {i}')\n",
    "\n",
    "    ansatz = AnsatzGenerator(FEATURE_DIM, reps=MAX_REPS, entanglement=ENTANGLEMENT)\n",
    "    initial_point = layerwise_training(ansatz, MAX_REPS, COBYLA(maxiter=50), q_instance)\n",
    "\n",
    "    qnn = TwoLayerQNN(\n",
    "        feature_map=feature_map,\n",
    "        ansatz=ansatz,\n",
    "        observable=GLOBAL_OPERATOR,\n",
    "        # input_params=feature_map.parameters,\n",
    "        # weight_params=anz.parameters,\n",
    "        quantum_instance=q_instance\n",
    "    )\n",
    "\n",
    "    model = TorchConnector(qnn, initial_weights=initial_point)\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.05)\n",
    "\n",
    "    # model, losses, accuracy_train, accuracy_test, weights = train_batch(\n",
    "    #     model, \n",
    "    #     MAX_ITER, \n",
    "    #     train_dataloader=train_dataloader, \n",
    "    #     val_dataloader=val_dataloader,\n",
    "    #     optimizer = optimizer, \n",
    "    #     loss_function = loss_function\n",
    "    #     )\n",
    "    \n",
    "    model, losses, accuracy_train, accuracy_test, weights = train(\n",
    "        model, \n",
    "        MAX_ITER, \n",
    "        X_train_t,\n",
    "        y_train_t,\n",
    "        X_val_t,\n",
    "        y_val_t,\n",
    "        optimizer = optimizer, \n",
    "        loss_function = loss_function\n",
    "        )\n",
    "\n",
    "    losses_2.append(losses)\n",
    "    accuracy_train_2.append(accuracy_train)\n",
    "    accuracy_test_2.append(accuracy_test)\n",
    "    weights_2.append(weights)\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(losses_2[i] for i in range(len(losses_2))).astype('float').to_csv(f'./Saves/LossFunction/m2.csv')\n",
    "pd.DataFrame(accuracy_train_2[i] for i in range(len(accuracy_train_2))).to_csv(f'./Saves/Scores/Train/m2.csv')\n",
    "pd.DataFrame(accuracy_test_2[i] for i in range(len(accuracy_test_2))).to_csv(f'./Saves/Scores/Test/m2.csv')\n",
    "for wr in range(len(weights_2)):\n",
    "    weight_record = pd.DataFrame(weights_2[wr]).astype('float')\n",
    "    weight_record.to_csv(f'./Saves/Weights/m2/sample_{wr}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a13c8f5b783206a43b73a673b4c249a5e5ee0a2cf865a54b80fb656bbdf8626"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
